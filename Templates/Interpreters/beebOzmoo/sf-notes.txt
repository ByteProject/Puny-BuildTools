Looking at profiling for smalldyn benchmark 3.7f vs 5.3:
- is it possible read_byte_at_z_address is being called more now? Not too clear, but the lda (mempointer),y when it's the same 256 byte segment is hot in 5.3 and not in 3.7f
- read_next_byte (which calls read_byte_at_z_address unconditionally, and always has) is called way more often in 5.3 than 3.7f
- .read_new_byte inside read_byte_at_z_address is called *more* in 5.3, but not all that much (maybe twice as much, but it's not called all that much in the first place) - so it's the "same 256 byte segment" case which is potentially accounting for most of the lost performance
- read_byte_at_z_address is not called significantly more outside read_next_byte in 5.3 than 3.7f, so I think read_next_byte is a big part of the difference


Master 128, mode 7:
- before *FX19 use benchmark: $0049bd
- with *FX19 use: $004c0d
- so a 3.1% increase in runtime
- absolute increase is 592
- !if-ing out the osbyte to read vdu status gives $4c0f ($4c0d another time), i.e. negligible
- !if-ing out the VSYNC osbyte (I did see the solid block flicker in at one point) gives $49bf, i.e. back where it was before, as we'd expect
mode 3:
- before *FX19 $005850
- with $00592b
- so just under 1% increase in runtime
- absolute increase is 219
mode 6:
- before $0051b2
- with $0051f2
- 0.3% increase in runtime
- absolute increase is 64


Executable compression:
- OZMOOB load-and-decompress 1.59, load 1.29 (timed on M128)
- OZMOOSH l-a-d 1.74, load 1.42 (timed on M128)
- OZMOOE l-a-d 2.8, load 2.3 (timed with stopwatch using Elkulator in normal mode)
Without executable compression, timed in same way:
- OZMOOB load 1.81
- OZMOOSH load 1.74
- OZMOOE load 2.8
(OZMOO2P is awkward to time as it corrupts BASIC workspace)
So it looks like the compression doesn't hurt, since load-and-decompress times are pretty much the same as load times for uncompressed (and thus larger) files, and for OZMOOB (probably due to its screen hole) it's actually a small improvement. And of course given there's no time penalty compression is a win, since it frees up more space on the disc.


On b-em, a B with DFS 2.26 and drive noises on takes 3.02 seconds to load a 20K mode 2 splash screen.
lzsa1 compresses that down to 12.3K, which might cut the load time down to 1.86 seconds (spurious precision). If we can decompress 17600 bytes per second we will break even in terms of user experience. That's 352 bytes per frame. Obviously faster is better, but looking at https://github.com/emmanuel-marty/lzsa (and bearing in mind I don't know platform the graph is from - a spectrum?), that suggests exomiser is a bit slower than ideal (but the faster variants would probably just about break even) - that fits with my intuition from the mode 1 NuLA slideshow stuff I did, where exomiser did seem to slow things down but was worth it as getting more images on the disc was a big win. Since saving disc space is useful here, I can probably afford to go up to lzsa2 compression, unless the decompression code is much bulkier.
- the compressed version takes 2.80 seconds to load and display the splash screen. *LOAD of the SPLASH executable takes 1.79s. It's a shame we can't use the faster decompression code but we need backwards decompression because the screen RAM is at the top of memory and we don't have enough RAM to not overlap the buffers. Still, this isn't bad, it is saving disc space (8.5K after we allow for sector size rounding) and it is slightly faster than loading the raw uncompressed file. (I am using lzsa2 optimised for ratio; I suppose I could push things a bit to improve the performance, but saving disc space is good too and given the compressed version is still faster than uncompressed I'm not too inclined to sacrifice disc space for a tiny bit of speed.)



M128+normal 2P, --no-turbo $0029ff
M128+normal 2P, (turbo supported but not in use of course) $002a2b


B 144K SWR $0047ef
no-dynmem-adjust $004b14 - if this is to be believed a big improvement from the dynmem promotion

256K turbo, M128 host, mode 7, drive noises off, no-dynmem-adjust $00244b ($00244a)
ditto but with default dynmem-adjust $0021ec ($0021ec) - so 6.5% faster due to more memory access going via "dynmem" not "vmap"
4MHZ "turbo" copro just for fun, otherwise ditto $0023f8 ($0023f9) - so with the dynmem adjust, the 3MHz 256K turbo beats the 4MHz 64K "turbo"
3MHz 64K copro $2b5c - so 21.7% faster with a 3MHz 256K
- OK, looking at tubes[] array in b-em model.c, it looks like the turbo copro is being clocked (correctly?) at 4MHz - but even so, as per the "just for fun" result above, there is an improvement, it's just not as big


b-em, 256K turbo co-pro, no host cache, mode 7, drive noises off - $002300
Normal 6502 co-pro, host cache (144K model B), $0022bf
Ditto but host cache 64K M128 $002b2e
Ditto but host an unexpanded model B $0029fd
- so at least for the benchmark a turbo-co pro doesn't help much if you have plenty of host cache (which is maybe a testament to how fast copying data over the tube can be, combined with the fact that at least for the benchmark there's a fair amount of vram cache so we can probably maintain a pretty decent working set and don't need to hit the host cache much) - I suspect for larger games the turbo copro might show more a gain, because there's essentially no overhead to access 128K read-only memory plus maybe 40-50K (guessing) of dynamic memory, plus you can still (once I implement it) get host cache benefits on top of that

b-em, drive noises on, BBC B, 8271, DFS 0.9, 16K sideways RAM - benchmark completes at - no, it locked up in DFS code (&8CD7-DA) - forgot to note move number, *maybe* 251 but don't trust that
DFS 1.2 - no, locked up at move 253, again in DFS (ACAE/ACB1)
OK, let's turn tube on (then I can compare this with BeebEm, which can't handle BBC B no shadow), still DFS 1.2 (16K SWR has disappeared but that's a b-em glitch I think) - OK, it has locked up at move 380 - so I am thinking this is the b-em (?) problem I noticed a while back - it's stuck in DFS again at ACAE/ACB1 - yes, with BeebEm, B with 2P, DFS 1.2, no SWR completes fine, so I'm not going to worry about this too much

New build system:
- I might have to rewrite in ruby eventually but I want to start from a good Python version if I do that
- need to be careful not to make it too clever
- I need it to be semi-modular so e.g. you can do a tube-only build and the loader still works. Ideally I also want to be able to do "special config X" (where X might be "Integra-B shadow RAM for game data" or "Watford sideways RAM on a B" or "AQR") builds where the user says "I'm happy for the build to support only X not other things" and we can to some extent assume the presence of X instead of trying to auto-detect it in ways that might be unreliable. Even more ideally it would be possible (where auto-detection is feasible) to add those config X builds to a disc supporting more vanilla configs, but that isn't a high priority unless it can be supported easily. I would imagine that a build for X would support plain X and X+tube though, unless of course you specified tube-only.
- the loader BASIC code should maybe support a #ifdef style mechanism rather than the build script substituting in fragments of BASIC which rely on knowledge of the loader code. This is probably clearer, cleaner and simpler - especially if we are making the loader support some configs not being present on a particular image. Apart from conditionally including/excluding blocks of code, I could also use ${FOO} macro-ish substitution to include filenames for binaries which would allow the build script to indicate which surface of a DFS disc they were on.
- DFS/ADFS image generation should probably be handled via classes which don't have to be full-featured but which present a "nice" interface which could in theory be extended to be full-featured, without being overly ornate. I suppose really the important thing is they're clean-ish. They should probably have some kind of common interface of course.
- Handling of double-sided DFS disc images is currently rather hacky. Maybe the DFS image should wrap *both* surfaces rather than having separate images per surface, not sure though.
- It would be nice to be smarter about distributing the miscellaneous files over both surfaces of a double-sided DFS disc.
- It would be nice (but not essential - C64 build system doesn't do this) if we could automatically upgrade to a double-sided disc if the stuff won't fit on a single-sided disc. Doing things inside a function rather than all at the top level might be all it takes to support this easily.
- I should probably at least start by trying to rewrite/tidy up bits of the existing build script in order to avoid the temptation to bash out shoddy code to "get things working", which is how I got into this mess in the first place. :-)
- the really core thing that needs to be flexible and clear is building of executables
  - need to be able to handle the "try this, then try that, then try this, then compared that one and that one, then tweak it, then pick that tweaked on" stuff in as clean and general a way as possible
  - I do like the idea the current code has of having an Executable object which represents the result of trying to build something, and that it can be None if we couldn't (or decided not to, e.g. the user said not to both) build something
  - I would like CACHE2P/FINDSWR to be Executable objects themselves
  - we might potentially build up a "pseudo disc image" in memory by adding Executable objects to a list, then towards the end we would turn that into a set of add_file() calls on actual DFS/ADFS disc image objects, at which point we could potentially handle "which side shall I put this on?" for DFS double-sided more easily (e.g. always put !BOOT and LOADER on side 0, but all the other non-data files go on whichever surface has most free space, and the loader BASIC program uses ${FOO} variables for filenames so it knows where they are - we'd want to put FINDSWR on first as it's always used on startup by loader, so it might always go on side 0, and we'd probably want tp ensure CACHE2P comes "between" the loader and the Ozmoo2p binary, it might be that we treat those two as a unit and always put them (in that order) on the same surface.
  - having this "psuedo disc image" which is just a list of Executable objects would also probably be helpful in being able to sum up the sector-rounded sizes and saying "OK, this will not fit on a single-sided disc, let's automatically expand to double-sided before we start"
  - we do have a general pattern of having sets of assembly arguments which serve as a base, then we add extra options, then those form a new base with variations on them being tested
  - we have a general pattern of needing to generate label/binary/report names for variations on an executable by subtitution - if we can automate this in some way it would probably simplify and clarify quite a bit, I don't want to end up with super-verbose names but the names are not all that critical (though ideally they would be user-intelligible not e.g. just assigned some arbitrary integer values during the build) - perhaps we would just have standardised template to use e.g. "ozmoo_${VMEM:vmem}_${ACORN_SWR_BIG_DYNMEM::_sdyn}_${STARTADDR}"
  - doing the necessary extra build and chop-off-0 and append relocations work for ACORN_RELOCATABLE might also be something we could wrap up
  - when building up -D options, we should probably distinguish "Ozmoo executable args" like "-DTRACE_VM=1" from general args which might apply to FINDSWR/CACHE2P binaries - not a big deal, but keeping this cleaner is probably going to help share Executable objects for all executables not just Ozmoo variants
  - in general, it needs to be "normal" for building an Executable to fail and not require lots of exception-catching boilerplate - maybe this means there's a build() fn which may return None, or if (there probably isn't) there's any useful information from a failed build *or* (maybe the case) it's convenient to store "input" to the build in an Executable object, Executable objects may need to have a "failed" state they can be in, though I'd rather avoid this if I can
  - just maybe it's worth having the build script cache Executable objects so if we end up doing the same build multiple times we know (same command line arguments == same output, there's no randomness) we don't need to go and rebuild it
- just possibly I should work on the basis that I *won't* be able to tokenise BASIC at build time and therefore I can only do "simple" slice-and-dice type operations on the loader (e.g. delete chunks of lines based on walking the tokenised code line by line and recognising the contents of REM statements which contain #ifdef or whatever) and won't be able to generate arbitrary code in the loader - I'd rather not have this, but it *might* help avoid problems if I can't use beebasm in the final version - it may also be easy-ish to change variable names or the values assigned to variables in tokenised BASIC
- *maybe* I should generate 7-bit ASCII for the loader in order to avoid the hackery needed to treat it as binary on Python 3 - if I use the equivalent top-bit-clear codes for graphics characters this needn't bloat the BASIC code much, we'd still need to use CHR$() or VDU for "0-31" (which are treated differently by the OS and don't get put directly into screen memory; remember the SAA5050 is actually a 7-bit chip IIRC) i.e. 128-(128+31), but that's mainly the odd colour code which isn't going to account for a huge amount of characters even if someone does a custom graphics title page.
- maybe I should write a "word wrap with optional colour code prefix and blank space indent" procedure in the loader and use that for output rather than the current hacky "count spaces to make a simple string print over multiple lines correctly" approach - apart from being generally nicer, it helps make things work nicely when auto-generating code.
- maybe all of our command line arguments should be automatically converted into "#ifdef"-usable form in the loader, so that the build script doesn't need to special case things like AUTOSTART and DEFAULTMODE
- I should probably not make any attempt to parse disc images as input - the only possible reason to do this now is (I think) to get the tokenised BASIC generated by beebasm, but since that's the *only* file on the generated disc I think we can simply chop off the first two sectors and be done with it. (We could have a helper fn which wraps beebasm used as a BASIC tokeniser, then it would be easy to substitute an alternative tokeniser if desired; we aren't using beebasm as an assembler at all.)



Extending tube host cache to use SWR:
- I will move the "what SWR banks do we have" logic from the loader into findswr machine code DONE
- as part of that, I will make the count/list of banks output from the loader at $900-ish at a fixed address (I can get rid of the current fixed address output of course) DONE
- I hope all this doesn't cause loader to no longer fit in $900-aff, if it does I may need to take some steps of some kind DONE
- loader (in a cache-enabled build) will use an OS call to read those fixed addresses to show SWR details even if a second processor is detected DONE
- cache2p binary will use those fixed addresses (on the host, where it is anyway of course) to control its own SWR use


Timings with initial crude tube host cache prototype:
- with &2B blocks of host cache, b-em, mode 7, 3MHz copro, Master 128 host, drive noises on $003131
- with non-caching code (current acorn-wip branch, as released to stardot) $003f25
- OK, with &2D blocks of host cache and preloading implemented to use the host cache (the preload time *is* included in the final benchmark time, remember I am not subtracting off the initial time) $002ef8
- currently (with probably-good preloading) getting $2ef6
- with relocating cache, a B with DFS+ADFS completes in $2e8e and Master 128 in $2e28
- now with sideways RAM support, Master 128 does no disc access during game and completes in $2ae8
- with alpha 2.8 on dev MAME, an Electron with a 3MHz copro and 48K SWR completed benchmark in (approx; I started timing a few seconds in) 4:35-4:45, as measured by stopwatch (mode 6)


restart and checking for game disc:
- you're not "supposed" to not have the game disc in (at the moment)
- at least on DFS, if you take the game disc out except when you're told you can during a save, the game is likely to crash as DFS reads a block of game data from your non-game disc
- in some ways restart is no different, if you have sneakily left the wrong disc in you will get a crash then too (plus this is a pretty benign crash, as you wanted to restart *anyway*)
- I have some sympathy with the idea that a "knowldgeable" user (or just possibly we might *tell* the user if this is true) knows the game fits entirely into RAM and decides they can leave their save game disc in the drive all the time
- but as it stands the game will currently be checking for re-insertion of the game disc after a save, thwarting their attempt to be clever
- so I am thinking *either* we say "the game disc is meant to be in the drive except during saves" *or* we explicitly support a "you have enough RAM, you can remove the game disc after loading" feature and that disables the check for the game disc after a save
- if we expect the game disc to be in the drive all the time except when saving, restart doesn't really need a check for the game disc to be in
- if we sometimes allow the user to remove the game disc with our permission due to having lots of RAM, we should then do a check for the game disc on restart
- not saying I won't do any (modest) amount of work to support "game fits entirely in RAM" case, but I don't believe anyone who's given me feedback on stardot has that much RAM - short of AQR support (which I don't have yet), you'd probably need approximately 96K of sideways RAM (because there's maybe 16K of main RAM as well) to be having a chance of this occurring, although I suppose smaller games like Calypso do make this a bit more likely. "Game fits entirely in RAM" on SWR is kind of equivalent to "no vmem needed" on tube (except you can detect the latter at build time), so whatever I do I should make those behave equivalently.



Electron timing notes before and after change to allow the "wasted" main RAM to be used:
32K sideways RAM, no main RAM used - benchmark $00beca 
Experimental "use main RAM" code + 32K sideways RAM $00bb45 second try $00bb0f
- virtually no difference but I suspect interrupts get disabled a lot and the self-timing can't be trusted
Experimental - $1ae shown early on - I started a stopwatch when the screen cleared and intro started to scroll 
- move 95 at 4:05 approx
- move 228 at 9:36 approx
- move 340 at 12:42 approx
- new game prompt at 17:34 (move 449) - showed $00bb3c
Back to pure 32K sideways RAM, no main RAM used, stopwatch
- $1a3 shown early on
- at 4:05 on move 79
- move 95 at 4:43 approx
- at 9:36 on move 196
- at 14:15 on move 340
- new game prompt showed $00bec7 at 19:37
Conclusion
- so internal timings show <2% reduction in time from the extra ~5K (remember the benchmark text takes up a couple of K), while the stopwatch timings show a nearly 10.5% reduction (from a 15% increase in RAM)




load_suggested_pages performance:
- no drive noises (which would in reality dilute any overhead further, as a percentage)
- B+ 128K DFS mode 7
- current load_suggested_pages code has $2b0 at start of benchmark (confirmed)
- alpha 2.4 tag has $2af (confirmed)
- so there's no real difference, which is what I'd expected/hoped but good to confirm the bit of extra work being done between each readblocks call is not a significant overhead



Testing checklist:
- ADFS vs DFS
- single vs double-sided
- all four executables
- Python 2 vs Python 3
- small non-VMEM games on second processor as well as large VMEM games
- BBC B executable at &E00, &1900 and &1D00 (screen hole makes it very sensitive)
- really big game like Beyond Zork

Benchmarking:
- THIS IS BIGDYN BECAUSE NOT TAKEN ANY STEPS TO AVOID IT
- Master 128, mode 7, 64K sideways RAM, b-em, drive noises *off* $004988
- ditto but model B with 144K sideways RAM (so no drive access and maximum paging) $004a4e conf
- now adding jsr-to-rts in paging code
- M128 $004e69 1.0664
- model B 144K $004f12 $004f11 1.0641



Electron:
- as per git commit notes, "64k" auto-detected SWR build hangs on move 382 of benchmark
- forcing LOADER to only recognise one bank of SWR a) it is much slower (which is good, as it suggests we are "correctly" using multiple banks on the whole) b) the benchmark completes (FWIW showing time $019b41)
- forcing LOADER to take the first two banks of SWR identified (I think 3 & 5, but not sure) it hangs *much* earleir
- forcing LOADER to use just banks 3 and 7 - OK, I think that hangs in the same place as with 3 & 5, much earlier, at move 50
- OK, thanks to Pernod on stardot using Pegasus 400 DFS everything works!



Vmem notes (mostly looking at block aging stuff):
- I'll assume Z3 in these notes, where vmem_tick_increment == 2 and vmem_highbyte_mask = $01, but except for reduced resolution I don't think other versions are significantly different
- vmem_blockmask is always $fe=254 - this is used to allow for the fact a VM block contains two pages
- vmem_tick starts off at $e0
- vmem_tick doesn't move unless we need access to a block of data and it isn't in our VM cache, i.e. we need to read a block from disc and we therefore need a slot in our VM cache to store it in.
     - by this point we have already picked out the "oldest" entry so we can re-use that slot
        - in terms of picking out the oldest, we start looking at vmem_clock_index and entries have to be strictly older to replace the previous oldest candidate
        - vmem_clock_index is set to the index following the oldest candidate ready for the next search - I think doing this rather than always starting at index 0 might have some kind of effect on tie-breaking when multiple entries share exactly the same time stamp, but TODO: I really need to think this through - OK, recheck this later, but I think this is really a performance optimisation - if we started at 0 every time, we'd check lots of entries that were unlikely to be the oldest - no, but we always go round the full vmap *anyway*, so that can't be it - I suppose it does have an effect of spreading things out - if we always started at 0 blocks with joint-oldest age near 0 would always be paged out in preference to equally old blocks further down the map, whereas always starting at clock_index will tend to be "fairer" - I can see some value in this but it's not obvious to me that it is automatically a big win as I write this
    - vmem_tick is incremented by vmem_tick_increment
    - if vmem_tick wraps round, it gets set to $80 and the vmap entries are adjusted - the adjustment subtracts $80, if the result is negative the value is set to 0 (with an adjustment to preserve the non-tick-related high byte using vmem_highbyte_mask)
    - the newly read block is given the new value of vmem_tick (although debugging code will print the older value)
- if we need a block of data and it's already in the VM cache, vmem_tick does *not* change but that block's vmap entry is set to the current value of vmem_tick





Preopt and related stuff:
- I think the basics of this are not a big deal - start with an empty vmap, populate it and dump the contents when it's full
- the build system then needs to pass (presumably in order of first use) this into the binary so it can load the relevant blocks - the binary should probably sort these itself so as to seek efficiently across the disc, we can't sort them in the build system because only the binary knows how much RAM it actually has to work with, it may end up truncating the list of suggested blocks (e.g. suppose the preload blocks are 5, 6, 130, 4 in that order - if we have >=4 blocks of VM cache we want to load 4, 5, 6, 130, but if we have 3 blocks of VM cache we want to load 5, 6, 130 not 4, 5, 6 which we would do if the list were pre-sorted before truncation - note also depending on exactly how the setting of the initial age works, we may need to be careful not to lose information by doing this sort in order to do efficient loading)
- this may mean the binary needs to be capable of generating its own vmap, including doing linear interpolation or similar of the age for the entries it is generating - however, this is maybe awkward because although there is some possibility of "early" discardable init code running from story_space, the actual space in the Ozmoo stack is already pretty full - swr_shr_vmem has about 134 bytes free - other variants have more, maybe something could be moved out of the stack or optimised or whatever, but this does need consideration - adjust_dynamic_memory_inline is probably a modest chunk of the code taking the stack space on builds which have it, which isn't to say it's not worth it
- I have had a bit of a poke at the C64 code but I haven't studied it all that thoroughly and I may be missing something
- if the binary is now generating its own vmap, it may well be useful for the vmap to live in non-initialised RAM in 400-800 (something else can move out of there to compensate)
- if the vmap is going to live in non-initialised RAM, it may be a good idea to pick up the aborted "restart only reloads dynamic memory" change and preserve the vmap across restarts - although this is a bit debatable, as if you're restarting you might well want all the "start of game" blocks loaded in one go rather than what you happened to have - then again maybe you're restarting just to do a restore, but usually you would just do a restore of course
- it's possible an Electron implementation would affect generation of the vmap, as it may want to generate one with a "hole" in it for the 8K screen (to allow the ~4K below the screen to be used as VM cache), so I may want to postpone any implementation of any of this until I've decided if/how I am going to support the Electron.
- so sketching out (and ignoring Electron, which is probably not a big deal but let's keep it simple) how this would work:
	- the loader puts the suggestions into the vmap in the standard format (most "important" with newest ages, i.e. the ones loaded first during the preopt generation run which were *oldest* in that run and youngest in the vmap created by loader) - the list of suggestions is padded up to the full vmap size with missing and/or linear blocks (not too important, but they should probably be useful-ish entries as Acorn code doesn't I think like/want to grow vmap table, it starts off full)
	- the Ozmoo binary needs to count up how many blocks of vmem cache it can actually support - it probably does this already actually, as we need to set vmap_max_entries accordingly
	- the Ozmoo binary truncates the list of suggestions - this may happen implicitly as we set vmap_max_entries
	- the Ozmoo binary sorts the truncated list *using the raw game block address, not the address-with-timestamp* - this can't be done in the loader, because where the list gets truncated will affect the result - this isn't a big deal, but just note that the sort key is the value in vmap with the timestamp masked off, but of course we want to retain the full vmap entry with timestamp as we swap the blocks into order
	- the Ozmoo binary uses something like load_blocks_from_index to pull each vmap entry into memory, instead of just using readblocks to read from story_start up to the end of SWR (not needing to care about what's dynmem and what's vmem preload)




"Free memory":
- on all builds "a lot of" page 4 is really free (~113 bytes)
- non-2P builds have page 7 completely free
- we have scratch page on 2P and double scratch page on non-2P, we do need these but we could maybe use a bit more
- if necessary to free up "big" chunks of non-initialised low memory, we could of course move stuff out of there into the main Ozmoo binary
- there is also the up-to-511 bytes of padding to ensure the stack starts with the right alignment, though of course this is hard to use flexibly since it may disappear and we always need (even if it's just manual fiddling) some sort of fallback, and if we have somewhere to fall back to why would we not just use it all the time anyway?

Possible uses of free memory:
- extend game_data_filename space from 32 bytes
- maybe relocate part of vmem map into it - this needs to be initialised though
- maybe relocate some/all of 160 bytes of ACORN_HW_SCROLL buffer into it
- there's about 57 bytes of misc data scattered round the code which might be relocatable into this space, but that creates a bit of a gratuitous difference from C64 as it's mostly "upstream" data



ADFS:
I think we need to do *DISMOUNT before entering the save/restore prompt
Probably also necessary/a good idea to close the game data file before doing *DISMOUNT
When we're about to resume, we probably need to do *MOUNT, try to open the file (and if we succeed do the DFS-style CRC check), catch errors (making sure we close the file if we opened it, and do a *DISMOUNT as well) and repeat the whole process after the user presses a key.
- ****BUT**** *DISMOUNT will lose the current directory - which is not a good thing if we've been installed on a hard drive in some random directory. So maybe we need to detect that case and not fiddle with *DISMOUNT/*MOUNT if so. But what if the user goes and does *MOUNT 4 to save a game to a floppy? Admittedly this is not all that likely, but let's pretend. Will that mean we can't re-open our "DATA" (with no drive/dir prefix) file? I guess on a hard drive installation, if we're going to have to detect that case, we would simply keep the file open during a save/restore.
- can we detect hard drive? do we really want to? should we maybe instead keep the game data file open, but be ready to experience a "Channel" error when we do the CRC read and if so do *MOUNT and re-open it? But if we haven't done *DISMOUNT (and we're not trying to behave different on hard or floppy drive), the user has to go and do a *MOUNT manually if they've changed the disc, whereas if we *DISMOUNT they don't have to.
[FWIW, *is* (genuine Q) OSARGS A=&FF equivalent to *DISMOUNT? Might be useful, might not.]
[I don't know if there's an official way to detect a hard drive, *if* I want to go down that route, but reading the capacity - not free space - of the current drive would in practice be a good way to detect this. No hard drive is going to be <=640K.]

How about this?
- we try very hard not to implement an "ADFS" solution - we might in principle be running on some other filing system like Econet or something more exotic.
- so we don't go issuing *DISMOUNT or *MOUNT commands if we can possibly help it
- the loader determines the current path and stashes the full ":0.$.Foo.Bar.Ozmoo.Data" string somewhere in memory - note the drive number (awkward? what if we're on a file system without drive numbers?)
- the game closes the data file before save/restore so that *if* the user does something like *MOUNT 0 to change the disc, we don't get upset at having the file closed underneath us
- after the save/restore is complete the game re-opens the data file using that full path - this way if the user has deliberately or accidentally changed the current directory to wherever they want to save (or maybe they hacked the loader to do this, if they have a hard drive installation) we won't get upset or lose their setting for the next time they save/restore
- what if the user has taken the game disc away? Our open might fail with a Disc changed error or a Not found error, or the CRC check once we open it might reveal the disc isn't the one we want. Ideally we would offer a * prompt to let the user "do stuff" to fix the situation before retrying, we could maybe get away with issuing a *MOUNT 0 command.
- RESTART is maybe going to be problematic as we need to launch the binary from the right path and if the user's changed it we will fail

Alternative:
- maybe the loader stashes a command or commands somewhere in memory to do "post save/restore", and the loader tries to set something sensible up and an advanced user (i.e. one installing to hard drive or Econet or whatever) can tweak this if the loader doesn't do the right thing automatically, as the loader is BASIC.
- the loader might similarly stash a full restart path
- apart from executing the post save/restore commands, the game would "just" a) close the file before the save/restore b) open the data file using a stashed-by-the-loader path, check its contents via CRC. If b) fails, retry - although ideally we still need a command prompt loop.

So on save restore:
- close the file
- tell the user they can remove the game disc (but ideally don't do that if e.g. we're running from a hard drive)

After save restore:
- I don't think we can "just try" to see if the game disc is already there, as we can on DFS, because it's got a fair chance of causing some sort of "Disc changed" or "Not mounted" error and the user had no chance to do it - hmm, maybe we can/should *recognise* Disc changed and swallow it but "do something"???
- perhaps we can "just try" by doing an OPENIN on the game data file - it's always possible no disc is mounted, but probably only if the user did something and semi-invited this

If we *are* on ADFS using floppy:
- we need to close the game data before saving
- ideally we would *DISMOUNT too (this may avoid need for user to *MOUNT explicitly)
- we would tell the user they can remove the game disc
- they do * commands and save as normal
- once they've finished, we *ask* them to put the game disc back in and press SPACE - unless we can try to OPENIN the game data file and swallow any error - OK, so we try the OPENIN, if it succeeds we should check the CRC before considering it to be OK, if it fails we swallow the error and the user to put the game disc back in, at which point we do *MOUNT 0 ourselves and retry - I think we need to be opening the data file using a full path, so if the user did *MOUNT 1 we won't get upset

OK, let's look at it this way:
- if we're on a hard drive or Econet, it's "easy" - we just keep the game data file open (we need to be careful re path on restart, but that's all) and unless the user goes an *DISMOUNTs the hard drive or something we're fine. We may need to *not* do some stuff we would otherwise do for floppy (actual commands, or maybe printing messages about "removing/re-inserting the game disc"), but it's not really a big deal. It might be nice to allow * commands if things go wrong, but this will probably just fall out of stuff we need to do for floppy anyway.
- conceptually being on a hard drive but saving to a floppy is no different than being on a disc in drive 0 and saving to drive 1 - the user will be using *MOUNT some-other-drive, they may accidentally *DISMOUNT the drive we are on, etc.
- playing around on emulated master, using OPENIN (at least with a full path) appears to implicitly cause a mount, whereas LOAD "A" after a dismount gives a "No directory" error - the full path does appear to be the magic ingredient, it's not e.g. OPENIN vs LOAD which is important. Hmm, OPENIN does sometimes seem to work when LOAD doesn't, it's really inconsistent. But a full path with a drive number does appear to generate provoke ADFS to "have a go" rather than say "No directory". Also, of course, OPENIN can "fail" quietly by returning a zero channel, which I suspect is a key observation here. Giving the full path does appear to sidestep the whole "disc changed" error in general though, full path with drive number that is. This may well simplify things, if the loader obtains a full path with drive number and stashes it somewhere, we may be able to do open-read-block-0-and-crc without *expecting* to get any OS errors occurring, though we should ideally cope if they do.

For the nth time then:
- prior to save/restore we close the game data file
- if we're "on floppy", we *DISMOUNT and print "you can now remove the game disc" - hmm, this *DISMOUNT is nice in that I think it allows the user to swap the disc without needing to *MOUNT, *but* if the user is actually saving onto the game disc but would like to use a non-root directory, the *DISMOUNT will put them back in the root directory every time - so maybe we don't do the *DISMOUNT, at which point printing the "you can now remove the game disc" is a harmless and understandable quirk if the game is on hard drive, and we maybe don't even need to try to distinguish the two cases
- we let the user enter * commands etc as usual and do the save/restore
- we then re-open the game data file using a full path including a drive number - this may return a zero file handle, or we may read the data from the file and find the crc doesn't match - in either case we close the file, ask the user to put the game disc in and press space and then we try again - we may need to be careful here because the readblocks code will perhaps want to get in on the act with the retry, but we may want/need to be doing it ourselves - if we made readblock automatically close and re-open the file on error, it may well be we would not have any worries there
- if this model works, the loader needs to pass a) the full path of the game data file b) the full path of the Ozmoo binary (for restarts) to the Ozmoo binary somewhere in memory - we need this on all builds, not just SWR of course - if these paths are not allowed to be insanely long (the loader could object) we could probably squash them into page 4 and shrink the jmp_buf a bit more - the Ozmoo binary could have code to copy a single path (of the game data file) into the scratch page and hack the last bit after the "." to be its own executable name and do the *RUN from there, rather than the us needing to allocate two bits of memory for duplicate paths - this is a bit fiddly (=> more resident code), but it would also offer flexibility for DFS versions to have the executables on "arbitrary" sides of the disc, as long as the loader knows where they are




Benchmarks after reworking sideways RAM paging:

Baseline is 87f85b82608f6072ebeaebd0dfde488931e00c42 acorn-swr branch latest - this always uses "old-style no tweaks bigdyn":
Using b-em Master 128 mode with 64K sideways RAM in mode 7 for all tests
Drive noises off: $004d84 (repeat $004d83)
Drive noises on: $0051c2 (repeat $0051c2)

Comparison is latest acorn-wip 30ebfe64c647828c2716e0e3b49027d80479af11
bigdyn: 
Drive noises off: $0049a4 95.0% (repeat $0049a5)
Drive noises on: $004e0b 95.5% (repeat $004e0b)

smalldyn:
Drive noises off: $0045c0 90.0% (repeat $0045bf)
Drive noises on: $004a00 90.5% (repeat $004a00)




Register use analysis:

read_byte_at_z_address:
- returns value read in A
- upstream code looks like X has a pretty arbitrary value on exit
- upstream code will always (?) return with Y=0 but I don't believe any caller relies on that
- called by get_page_at_z_pc{,_did_pha}
- called by .initial_copy_loop in C64 disk code, but that immediately trashes A, X and Y after call
- called by read_next_byte, which explicitly preserves its caller's Y and doesn't use the Y=0 return
- called by .copy_table_common, which immediately does ldy #0 after calling it and trashes X a dozen or so instructions later
- no other callers
- so I think this is allowed to corrupt X and Y

get_page_at_z_pc{,_did_pha}:
- contains upstream code to explicitly preserve A and X
- upstream code explicitly returns with Y=0 and has a comment saying this is important
- calls read_byte_at_z_address
- called by C64 restore_game, which immediately does lda and ldx afterwards - there is no clear use of Y in the following code, and I am 99% confident it doesn't matter - it does an rts after which will go via make_branch_{true,false) and as discussed under read_next_byte_at_z_pc those will return control to the main loop
- called by inc_z_pc_page - this is used only by read_next_byte_at_z_pc and so I am pretty confident (see notes about that routine) that it is not necessary to have Y=0 for that
- called by z_init, which will trash A, X and Y not longer afterwards
- so I think this can maybe corrupt Y, provided read_next_byte_at_z_pc takes its own steps to ensure Y=0 (and I'm not sure that's actually necessary)

read_next_byte:
- returns value in A
- upstream code explicitly preserves caller's Y
- calls read_byte_at_z_address
- upstream code will return with the X value from read_byte_at_z_address, which is pretty arbitrary
- has many callers, not analysed
- so I think this is allowed to corrupt X

inc_z_pc_page:
- upstream code explicitly preserves A
- X and Y are altered iff get_page_at_z_pc_did_pha is called and alters them; it explicitly preserves X, so inc_z_pc_page will also preserve X, and it may (depending on whether get_page... is called) set Y to 0
- called only by read_next_byte_at_z_pc

read_next_byte_at_z_pc:
- returns result in A
- upstream code will always return with Y=0, because it sets Y=0 and *maybe* calls inc_z_pc_page, which will also set Y=0
- will preserve X
- has many callers, some of which rely on it preserving X
- I can't see any callers obviously depending on Y=0 but I can't be 100% sure there isn't something I'm missing - I am 95% confident having looked at the callers this isn't a problem, and in practice it really doesn't seem to break anything having Y!=0 on exit - OK, I've been over the callers in more detail, I am now 99% confident - the only possibly iffiness is make_branch_{true,false}, but I am pretty sure those return control to the main loop at rts and except vaguely-possibly for the not_normal_exe_mode case the main loop trashes Y fairly early on












On C64 the game normally keeps I/O+ROM paged in from $D000 upwards; copy_page
is used to copy RAM around with RAM paged in from $D000 upwards. The vmem_cache
stuff keeps a copy of high RAM pages currently in use in low RAM. This is
accessible RAM caching inaccessible RAM; it has nothing to do with optimising
disc access. On Acorn second processor builds this is irrelevant as there's no
paging. SF: I suspect this is also irrelevant on Acorn SWR builds, but I
need to investigate exactly how Z-machine non-dynamic memory is accessed to see
if we can ensure the correct SWR bank is paged in at all times without using
this caching mechanisms.

mempointer is read in get_page_at_z_pc immediately after calling read_byte_at_z_address - it is copied into z_pc_mempointer+1
ditto: in initial_copy_loop (REU code)
read_byte_at_z_address updates mempointer; a fast path relies on mempointer+1 being consistent with zp_pc_[lh]
read_byte_at_z_address updates zp_pc_[lh]
read_next_byte uses read_byte_at_z_address to read the address at z_address+[012], it then advances z_address+[012]
copy_table_common uses read_byte_at_z_address
zp_pc_[lh] are really only used in vmem.asm instead read_byte_at_z_address; I think they basically boil down to "what page of Z-machine memory does mempointer currently point to?" as an optimisation.

I'm struggling to see why we have both mempointer and zp_pc_mempointer.
I *think* zp_pc_mempointer is used for the Z-machine program counter, and this may be allowed to be "separate" to mempointer. I see that the *cache* support refuses to evict a cache page if zp_pc_mempointer is referring to it, but I don't see anything in the disc<->RAM stuff which would stop a vmem page being discarded if zp_pc_mempointer refers to it.
Yes, I think that's basically it
- mempointer is used for data access, and is also moved in the process of updating zp_pc_mempointer but that's OK because we don't rely on mempointer pointing to any one place except in the short term
- I still can't see anything protecting vmem pages being used by zp_pc_mempointer being evicted, although I suppose short of extreme memory pressure (which we might see on a hypothetical 32K RAM port) this won't happen - I could potentially tweak the vmem eviction code to check against zp_pc_mempointer - one possible way to do this easily would be just under .no_such_block to set the tick on the vmem block containing the PC (we could keep a copy of the index when we do the new-page stuff for z-pc) to a very recent value - then again it might well be as easy just to tweak the chosen code - ah no, it looks OK (of course), just under .no_such_block we have "; Skip if z_pc points here" so this is fine
- since only (?) read_byte_at_z_address can discard pages from vmem, mempointer is always going to be safe from eviction - read_byte_at_z_address is what sets mempointer

I suspect on a SWR build, we would keep the bank containing z_pc_mempointer paged in at all times. read_byte_at_z_address would be used for reads of data, and it would briefly page in the relevant bank. We would probably need to tweak the logic when the Z-machine PC is being moved and might call into read_byte_at_z_address so we don't get into a loop - actually it would probably be fine, but it would be silly to page back in the old PC bank just to revert to paging in the new PC bank in the caller of read_byte_at_z_address in the PC update code.
- we *could* use the C64-ish cache mechanism to avoid this, but I don't think we need to - the C64 presumably needs its high ROM banked in for kernal interrupts and keyboard reading and so forth, whereas the SWR on the Acorn isn't conflicting with the OS and we can safely keep it paged in all the time, it's just a question of ensuring we cope when the PC and "data" reads are from different SWR banks.

I think zp_pc_[lh] are *not* related to the Z-machine PC; they may be misnamed, or "pc" may stand for something else, or of course I might have the wrong end of the stick.



Loading a $C800 byte preload file using readblocks (2 pages at a time) took (b-em, drive noises on, timings via kernal_readtime):
Master Turbo OS 3.2 - 8.22s
DFS 0.9 with Turbo copro - 8.0s
DFS 2.26 with Turbo copro - 8.2s
compared with OSFILE:
Master Turbo OS 3.2 - 7.62s
DFS 0.9 with Turbo copro - 7.62s
DFS 2.26 with Turbo copro - 7.62s
- so it looks as though giving up PRELOAD and using readblocks to do the initial load (which would simplify things if I have two different-sized preloads and/or if I start interleaving across both sides of the disc) is not a significant performance hit



Biggish chunks of data for initial move to $400:
.jmp_buf max 257 bytes but in reality much smaller, not measured/analysed yet


Empirically working out .jmp_buf storage requirement: poking stack page full of $EA and running benchmark then examining stack afterwards, it looks like we never got down below $1E0 (and some of that may be interrupts below our stack, which don't count for .jmp_buf purposes).
Skimming the call code, it doesn't look like Z machine function calls build up state on the 6502 stack - this makes sense, as otherwise a save/restore wouldn't restore that state correctly. So I don't think a sufficiently convoluted program running on Ozmoo can't provoke higher-than-normal 6502 stack use.
